Intro to Inferential Statistics
===============================

Lesson 1: Introduction and Lesson 7 (Intro to Descriptive Statistics) Review
----------------------------------------------------------------------------
An approximate answer to the right problem is worth a good deal more than an exact 
	answer to an approximate problem.

What is Inferential Statistics?
a. Finding approximate answers
b. Make predictions about data
c. Run tests to find the accuracy of the model

Course Overview (Descriptive Statistics):
a. Research methods and visualizing data
b. Describing data
c. Normal Distribution

Course Overview (Inferential Statistics):
a. Estimation and Hypothesis testing
b. t-tests and ANOVA
c. Relationships between two variables

Central Limit Theorem:
a. Mean of sampling distribution = Mean of population distribution
b. Relationship of standard deviation of sampling distribution (SE) and standard
	deviation of population (Ïƒ) is given by SE = Ïƒ / sqrt(sample size)

Central Limit Theorem helps in determining the probability of randomly selecting
	a sample with mean at least x.

--------------------------------------------------------------------------------

Lesson 2: Estimation
--------------------
Point estimation involves the use of sample data to calculate a single value 
	which is to serve as a "best guess" or "best estimate" of an unknown 
	population parameter.

Approximately 95% of sample means falls within 2Ïƒ/sqrt(n) of the population mean.

Margin of error (Error margin): 2Ïƒ/sqrt(n) = 2 * Standard Error (SE)
where Standard Error is standard deviation of sampling distribution.

In general,
Error margin = z * Standard Error

Population mean = (sample mean - error margin, sample mean + error margin)
(95% confidence interval for the mean)

95% of data falls under 2 SD of mean
95% of data falls under 1.96 SD of mean.

In other words:
In a sampling distribution, 95% of sample means fall within 1.96 standard errors
	from the population mean.

Margin of error: 1.96 * Ïƒ/sqrt(n) = 1.96 * Standard Error (SE)
where Standard Error is standard deviation of sampling distribution.

In general, we can say with 95% confidence that
Point Estimate of population mean = Sample mean
Interval Estimate of population mean = (sample mean - error margin, 
										sample mean + error margin)
 
Using z-table:
98% of sample means falls within 2.33 standard errors from the population mean.

+/- 2.33 are the critical values of z for 98% confidence.
+/- 1.96 are the critical values of z for 95% confidence.

Confidence intervals are used for estimating actual population parameters.

--------------------------------------------------------------------------------

Lesson 3: Hypothesis testing
----------------------------
If the probability of getting a particular sample mean is less than alpha, it is
	"unlikely to occur".

Levels of Likelihood (Alpha levels):
1 .05 (5%)
2 .01 (1%)
3 .001 (0.1%)

Less than alpha levels is usually considered unlikely.

Z-Critical value: Above which the probability of getting a particular sample mean
	is less than alpha.
Critical region: Region above Z-Critical value.

1.65 z-score marks the cutoff of 0.05 (5%) critical region
2.33 z-score marks the cutoff of 0.01 (1%) critical region
3.08 z-score marks the cutoff of 0.001 (0.1%) critical region

For e.g
z score of 1.82, then sampling mean is significant at p < 0.05

Z score of sample mean is calculated by using sampling distribution as a reference.

Two-tailed critical values at alpha = 0.05 (5%): +/- 1.96, since it got distributed
	in both the ends equally (2.5% each).

Two-tailed critical values at alpha = 0.01 (1%): +/- 2.57, since it got distributed
	in both the ends equally (0.5% each).

Two-tailed critical values at alpha = 0.001 (0.1%): +/- 3.32, since it got distributed
	in both the ends equally (0.05% each).

Ho(null hypothesis): Population mean = Sample mean (Population mean after intervention)
Ha(alternative hypothesis): Population mean < Sample mean
							Population mean > Sample mean
							Population mean != Sample mean

For null hypothesis to be true, sample mean will lie outside the critical region.
But, we can't prove that null hypothesis is true, so we only obtain invidence to
	reject the null hypothesis.

For e.g
A population parameter
population mean: 7.47
SD: 2.41

Sample of 30:
sample mean: 8.3
SE: 2.41/sqrt(30)
z-score = sample mean - population mean / SE = 1.89

At alpha = 0.05, we failed to reject the null for above example because z-score (1.89)
	is less than z-critical value of 1.96 for two-tailed hypothesis test.

Standard deviation of sampling distribution is standard error. SD/sqrt(n)

Hypothesis test:
Ho: Population mean = Population mean after intervention
Ha: Population mean >!=< Population mean afer intervention

One-tailed hypothesis testing is used to get directional treatment.
Two-tailed hypothesis testing is used to get non-directional treatment.

In general, two-tailed tests are more conservatives.

What does it mean to reject the null hypothesis?
a. Our sample mean falls within the critical region.
b. The z-score of our sample mean is greater than the critical value.
c. The probability of obtaining the sample mean is less than the alpha value.

Reject the null: p < 0.05

Hypothesis testing is prone to misinterpretations:

If we reject the null hypothesis, but it's actually true, then it is type 1 error.
If we failed to reject the null hypothesis, but it's false, then it is type 2 error.

--------------------------------------------------------------------------------

Lesson 4: t-Tests Part 1
------------------------
Z-test (Hypothesis testing) works when we know population parameters (ð, Ïƒ).

But, when we don't have population parameters, we can still estimate population
	parameters using samples.

Estimate of population SD using sample SD using Bessel's Correction:
SD = sqrt(Î£ (xi - xÌ…)^2 / (n-1))

For sampling distribution where SE is not calculated using central limit theorm,
	it is known as t-distribution.

In general,
In probability and statistics, Student's t-distribution (or simply the t-distribution) 
	is any member of a family of continuous probability distributions that arises 
	when estimating the mean of a normally distributed population in situations 
	where the sample size is small and population standard deviation is unknown.

The t-distribution is symmetric and bell-shaped, like the normal distribution, 
	but has heavier tails, meaning that it is more prone to producing values that 
	fall far from its mean.

For t-distribution, when sample size (n) increases:
a. SE decreases
b. t-distribution approaches a normal distribution.

Degrees of Freedom:
You have n numbers that must sum to 10?
Degrees of Freedom = n-1

For sampling distribution, if we already know the mean, then there are n-1 degrees
	of freedom to choose the sample.

For sample size n, degrees of freedom is n-1.

t-table:
Unlike z-table, t-table tells critical values in the body.
Column are the area under right tail.
Rows represents the degrees of freedom.
On x-axis, we have t-values.

One sample t-test:
t = (xÌ… - ð) / (SE/sqrt(n)) where SE is calculated using Bessel's Correction.

Larger the t-statistics, lower the probability of obtaining t-statistics, larger
	the difference between sample mean and population mean.

Cohen's d (Effect Size Measure):
Standardized mean difference that measures the distance between means in standarized
	units.

In general,
Cohen's d is defined as the difference between two means divided by a standard 
	deviation for the data

Cohen's d: (xÌ… - ð) / s
Bigger the Cohen's d, more probability is that sample comes from new distribution.

Confidence Interval: (Sample mean - t-statistic * SE, Sample mean + t-statistic * SE)
where SE = S / sqrt(n) for t-statistic
and t-statistic is 2.064 for 95% confidence.
and t-statistic * SE is also known as margin of error

Dependent t-test for paired samples:
Within subject designs
	a. Two conditions
	b. Pre-test, post-test
	c. Growth over time

Two samples (xi, yi) corresponds to (di) where di = |xi-yi|, and we use t-tests on d.
di is used to calculate SE along with Bessel's Correction.
