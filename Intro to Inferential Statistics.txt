Intro to Inferential Statistics
===============================

Lesson 1: Introduction and Lesson 7 (Intro to Descriptive Statistics) Review
----------------------------------------------------------------------------
An approximate answer to the right problem is worth a good deal more than an exact 
	answer to an approximate problem.

What is Inferential Statistics?
a. Finding approximate answers
b. Make predictions about data
c. Run tests to find the accuracy of the model

Course Overview (Descriptive Statistics):
a. Research methods and visualizing data
b. Describing data
c. Normal Distribution

Course Overview (Inferential Statistics):
a. Estimation and Hypothesis testing
b. t-tests and ANOVA
c. Relationships between two variables

Central Limit Theorem:
a. Mean of sampling distribution = Mean of population distribution
b. Relationship of standard deviation of sampling distribution (SE) and standard
	deviation of population (σ) is given by SE = σ / sqrt(sample size)

Central Limit Theorem helps in determining the probability of randomly selecting
	a sample with mean at least x.

--------------------------------------------------------------------------------

Lesson 2: Estimation
--------------------
Point estimation involves the use of sample data to calculate a single value 
	which is to serve as a "best guess" or "best estimate" of an unknown 
	population parameter.

Approximately 95% of sample means falls within 2σ/sqrt(n) of the population mean.

Margin of error (Error margin): 2σ/sqrt(n) = 2 * Standard Error (SE)
where Standard Error is standard deviation of sampling distribution.

In general,
Error margin = z * Standard Error

Population mean = (sample mean - error margin, sample mean + error margin)
(95% confidence interval for the mean)

95% of data falls under 2 SD of mean
95% of data falls under 1.96 SD of mean.

In other words:
In a sampling distribution, 95% of sample means fall within 1.96 standard errors
	from the population mean.

Margin of error: 1.96 * σ/sqrt(n) = 1.96 * Standard Error (SE)
where Standard Error is standard deviation of sampling distribution.

In general, we can say with 95% confidence that
Point Estimate of population mean = Sample mean
Interval Estimate of population mean = (sample mean - error margin, 
										sample mean + error margin)
 
Using z-table:
98% of sample means falls within 2.33 standard errors from the population mean.

+/- 2.33 are the critical values of z for 98% confidence.
+/- 1.96 are the critical values of z for 95% confidence.

Confidence intervals are used for estimating actual population parameters.

--------------------------------------------------------------------------------

Lesson 3: Hypothesis testing
----------------------------
If the probability of getting a particular sample mean is less than alpha, it is
	"unlikely to occur".

Levels of Likelihood (Alpha levels):
1 .05 (5%)
2 .01 (1%)
3 .001 (0.1%)

Less than alpha levels is usually considered unlikely.

Z-Critical value: Above which the probability of getting a particular sample mean
	is less than alpha.
Critical region: Region above Z-Critical value.

1.65 z-score marks the cutoff of 0.05 (5%) critical region
2.33 z-score marks the cutoff of 0.01 (1%) critical region
3.08 z-score marks the cutoff of 0.001 (0.1%) critical region

For e.g
z score of 1.82, then sampling mean is significant at p < 0.05

Z score of sample mean is calculated by using sampling distribution as a reference.

Two-tailed critical values at alpha = 0.05 (5%): +/- 1.96, since it got distributed
	in both the ends equally (2.5% each).

Two-tailed critical values at alpha = 0.01 (1%): +/- 2.57, since it got distributed
	in both the ends equally (0.5% each).

Two-tailed critical values at alpha = 0.001 (0.1%): +/- 3.32, since it got distributed
	in both the ends equally (0.05% each).

Ho(null hypothesis): Population mean = Sample mean (Population mean after intervention)
Ha(alternative hypothesis): Population mean < Sample mean
							Population mean > Sample mean
							Population mean != Sample mean

For null hypothesis to be true, sample mean will lie outside the critical region.
But, we can't prove that null hypothesis is true, so we only obtain invidence to
	reject the null hypothesis.

For e.g
A population parameter
population mean: 7.47
SD: 2.41

Sample of 30:
sample mean: 8.3
SE: 2.41/sqrt(30)
z-score = sample mean - population mean / SE = 1.89

At alpha = 0.05, we failed to reject the null for above example because z-score (1.89)
	is less than z-critical value of 1.96 for two-tailed hypothesis test.

Standard deviation of sampling distribution is standard error. SD/sqrt(n)

Hypothesis test:
Ho: Population mean = Population mean after intervention
Ha: Population mean >!=< Population mean afer intervention

One-tailed hypothesis testing is used to get directional treatment.
Two-tailed hypothesis testing is used to get non-directional treatment.

In general, two-tailed tests are more conservatives.

What does it mean to reject the null hypothesis?
a. Our sample mean falls within the critical region.
b. The z-score of our sample mean is greater than the critical value.
c. The probability of obtaining the sample mean is less than the alpha value.

Reject the null: p < 0.05

Hypothesis testing is prone to misinterpretations:

If we reject the null hypothesis, but it's actually true, then it is type 1 error.
If we failed to reject the null hypothesis, but it's false, then it is type 2 error.

--------------------------------------------------------------------------------

Lesson 4: t-Tests Part 1
------------------------
Z-test (Hypothesis testing) works when we know population parameters (𝝁, σ).

But, when we don't have population parameters, we can still estimate population
	parameters using samples.

Estimate of population SD using sample SD using Bessel's Correction:
SD = sqrt(Σ (xi - x̅)^2 / (n-1))

For sampling distribution where SE is not calculated using central limit theorm,
	it is known as t-distribution.

In general,
In probability and statistics, Student's t-distribution (or simply the t-distribution) 
	is any member of a family of continuous probability distributions that arises 
	when estimating the mean of a normally distributed population in situations 
	where the sample size is small and population standard deviation is unknown.

The t-distribution is symmetric and bell-shaped, like the normal distribution, 
	but has heavier tails, meaning that it is more prone to producing values that 
	fall far from its mean.

For t-distribution, when sample size (n) increases:
a. SE decreases
b. t-distribution approaches a normal distribution.

Degrees of Freedom:
You have n numbers that must sum to 10?
Degrees of Freedom = n-1

For sampling distribution, if we already know the mean, then there are n-1 degrees
	of freedom to choose the sample.

For sample size n, degrees of freedom is n-1.

t-table:
Unlike z-table, t-table tells critical values in the body.
Column are the area under right tail.
Rows represents the degrees of freedom.
On x-axis, we have t-values.

One sample t-test:
t = (x̅ - 𝝁) / (SE/sqrt(n)) where SE is calculated using Bessel's Correction.

Larger the t-statistics, lower the probability of obtaining t-statistics, larger
	the difference between sample mean and population mean.

Cohen's d (Effect Size Measure):
Standardized mean difference that measures the distance between means in standarized
	units.

In general,
Cohen's d is defined as the difference between two means divided by a standard 
	deviation for the data

Cohen's d: (x̅ - 𝝁) / sd
Bigger the Cohen's d, more probability is that sample comes from new distribution.

Confidence Interval: (Sample mean - t-statistic * SE, Sample mean + t-statistic * SE)
where SE = S / sqrt(n) for t-statistic
and t-statistic is 2.064 for 95% confidence.
and t-statistic * SE is also known as margin of error

Dependent t-test for paired samples:
Within subject designs
	a. Two conditions
	b. Pre-test, post-test
	c. Growth over time

Two samples (xi, yi) corresponds to (di) where di = |xi-yi|, and we use t-tests on d.
di is used to calculate SE along with Bessel's Correction.

--------------------------------------------------------------------------------

Lesson 5: t-tests Part 2
------------------------
Effect Size:
In experimental studies, it is size of treatment's effect.
In non-experimental studies, it is refer to strength of relationship between variables.

In z-test and t-test, effect size is calculated using mean difference (x̅ - 𝝁)

Types of effect size measures:
a. Difference Measures
	i. Mean difference
	ii. Standardized differences (Cohen's d)
b. Correlation measures (r2)

r2: proportion of variation in one variable that is related to (explained by)
	another variable.

Statistical Significance:
Rejected the null hypothesis
Results are not likely due to chance.

Meaningfulness of Results:
1. What was measured?
2. Effect Size
3. Can we rule out random chance?
4. Can we rule out alternative explanations? (Lurking Variables)

Cohen's d:
Effect size measure calculated by standardized mean difference.

d = (x̅ - 𝝁) / SD

Correlation measures:
Coefficient of Determination (r2)
a. Value ranges from 0-1

In t-tests:
r2 = t^2 / (t^2 + df)
where
t: t-statistics
df: degree of freedom

Results Selection:
a. Descriptive Statistics
	i. in text
	ii. in graph
	iii. in tables
b. Inferential Statistics
	i. hypothesis test (alpha level)
		1. kind of test
		2. test statistic
		3. df
		4. p-value
		5. direction of test
	ii. Confidence Intervals
		1. Confidence level
		2. Lower limit
		3. Upper limit
		4. Confidence interval for what?
	iii. Effect size measures
		1. Difference measures
		2. Correlation measures

APA Style for writing inferential statistics result.
For e.g 

For hypothesis test (t-test):
t(df) = x.xx, p = 0.xx, direction

For Confidence Interval:
Confidence interval on asdf difference, P% CI = (x, y)

Full one-sample t-test:
U.S families spent an average of 151 dollars per week on food in 2012.
Food Now company wants to reduce spent per week by introducing cost saving programs.
They found average spent for 25 people having standard deviation 50 is 126.
How significant is their cost saving programs?

What is the dependent variable?
Amount of money spent per week on food.

What is the treatment?
Cost saving program

What is the null hypothesis?
The program did not change the cost of food.

What is the alternative hypothesis?
The program reduced the cost of food.

Ho: 𝝁(program) >= 151
Ha: 𝝁(program) < 151

What type of test?
one tailed in -ve direction

Degrees of freedom for sample size (25)
df = n - 1 = 24

tcritical at alpha = 0.05 for -ve one tailed test
-1.711

SE = SD / sqrt(n) = 50 / sqrt(25) = 10

Metrics:
Mean Difference = -25
Compute t-statistic:
t-statistic = mean difference / SE = -25/10 = -2.5

Draw the t-distribution
Place tcritical on distribution
Shade critical region
Place t-statistic on t-distribution

Does t falls in critical region?
Yes

What is p value of t?
p < .05
p = .00985 = 0.01

Are these results statistically significant?
Yes

Are the results meaningful?
It depends

Cohen's d = -0.050

r2 = .21 = 21%
It shows that 21% is due to cost saving program.

Compute margin of error for 95% CI = 20.64
Compute the 95% CI for the mean = (105.36, 146.64)
